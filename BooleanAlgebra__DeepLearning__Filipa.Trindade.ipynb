{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Script for train and inference of an LGN and an FFN  \n",
    "\n",
    "Inference and training functions based on /experiments/main.py from https://github.com/Felix-Petersen/difflogic\n",
    " \n",
    "Author: Filipa Trindade\n",
    "\n",
    "BOOLEAN ALGEBRA: FROM DIGITAL CIRCUITS TO DEEP LEARNING APPLICATIONS Â© 2024 by Filipa Trindade Coito is licensed under CC BY-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from results_json import ResultsJSON\n",
    "from difflogic import LogicLayer, GroupSum, CompiledLogicNet, PackBitsTensor\n",
    "import difflogic_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder for training iterations and path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for each training iteration\n",
    "# Define timestamp for the folder's name\n",
    "time_string = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Folder for LGN trainings\n",
    "path = 'trained_models' + '/' + time_string\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "# Folder for FFN trainings\n",
    "path_ffn = 'trained_models_ffn' + '/' + time_string\n",
    "if not os.path.exists(path_ffn):\n",
    "    os.makedirs(path_ffn)\n",
    "\n",
    "\n",
    "# Path to the dataset\n",
    "datset_path = 'dataset_new/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of model parameters (LGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictonary of model parameters\n",
    "args = {\n",
    "        # Run ID\n",
    "        'id': 10, \n",
    "        # Tau Parameter for GroupSum\n",
    "        'tau': 20,\n",
    "        # Batch size\n",
    "        'batch_size': 4, \n",
    "        # Adam optimizer's learning rate\n",
    "        'learning_rate': 0.0001, \n",
    "        # Max. n. of epochs\n",
    "        'epochs': 200,\n",
    "        # Early-stopping's patience criteria\n",
    "        'patience': 4,\n",
    "        # n. of neurons in each layer\n",
    "        'num_neurons': 90000, \n",
    "        # n. of logic layers \n",
    "        'num_layers' : 4,\n",
    "        # n. of pictures on the val. set\n",
    "        'val_size': 316, \n",
    "        # n. of pictures on the test set\n",
    "        'test_size': 316, \n",
    "        # n. of grey-levels (n_levels) \n",
    "        'num_niveis': 7, \n",
    "        # Entry dimensions for the pictures in the dataset (40x40)\n",
    "        'img_size' : 40,\n",
    "        #Seed\n",
    "        'seed': 952\n",
    "       }\n",
    "# Creates the file log_args.txt and saves our args. dictionary\n",
    "with open(os.path.join(path, 'log_args.txt'), 'w') as file:\n",
    "    file.write(json.dumps(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Seed\n",
    "torch.manual_seed(args['seed'])\n",
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and resize the train pictures\n",
    "# Define a lambda function to subdivide the greyscale in a binary tensor with num_niveis elements \n",
    "trans_greyscale_in_binary = lambda x: torch.cat(\n",
    "    [\n",
    "        (x > ((i + 1) / (float(args['num_niveis']) + 1))).float() \n",
    "        for i in range(args['num_niveis'])\n",
    "    ], dim=0)\n",
    "\n",
    "\n",
    "# Define train pictures\n",
    "train_images = ImageFolder(\n",
    "    root=datset_path + '/train',\n",
    "    transform=transforms.Compose([\n",
    "        # Resize the pictures\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),  \n",
    "        # Transform to greyscale\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        # Convert to tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Applies the function previously defined\n",
    "        torchvision.transforms.Lambda(trans_greyscale_in_binary)\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_images_ff = ImageFolder(\n",
    "    root=datset_path + '/train',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),  \n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# Define test\n",
    "test_images = ImageFolder(\n",
    "    root=datset_path + '/test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        torchvision.transforms.Lambda(trans_greyscale_in_binary)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# Define test\n",
    "test_images_ff = ImageFolder(\n",
    "    root=datset_path + '/test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# Define validation\n",
    "val_images = ImageFolder(\n",
    "    root=datset_path + '/val',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        torchvision.transforms.Lambda(trans_greyscale_in_binary)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# Define validation\n",
    "val_images_ff = ImageFolder(\n",
    "    root=datset_path + '/val',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((args['img_size'], args['img_size'])),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the folders in Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_images, batch_size=args['batch_size'], shuffle=True, drop_last=True, num_workers = 4)\n",
    "test_loader  = torch.utils.data.DataLoader(test_images, batch_size=int(1e6), shuffle=False, num_workers = 4)\n",
    "val_loader   = torch.utils.data.DataLoader(val_images, batch_size=int(1e6), shuffle=False, num_workers = 4)\n",
    "\n",
    "results = ResultsJSON(eid=args['id'], path='./results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions for train and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions for train and inference\n",
    "def train(model, x, y, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    A step of SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    x = model(x)\n",
    "    loss = loss_fn(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def eval(model, loader, mode):\n",
    "    \"\"\"\n",
    "    Use the model to develop the mean of the output's tensor that \n",
    "    originates from applying the model to a set of pictures that come from the loaders\n",
    "\n",
    "    Because we are dealing with a binary classification problem, the returned value \n",
    "    corresponds directly to the accuracy of the set.\n",
    "\n",
    "    Taken from experiments/main.py from https://github.com/Felix-Petersen/difflogic\n",
    "    \"\"\"\n",
    "    orig_mode = model.training\n",
    "    with torch.no_grad():\n",
    "        model.train(mode=mode)\n",
    "        res = np.mean(\n",
    "            [\n",
    "                (model(x.to('cuda').round()).argmax(-1) == y.to('cuda')).to(torch.float32).mean().item()\n",
    "                for x, y in loader\n",
    "            ]\n",
    "        )\n",
    "        model.train(mode=orig_mode)\n",
    "    return res.item()\n",
    "\n",
    "def packbits_eval(model, loader):\n",
    "    \"\"\"\n",
    "    Same thing as the eval but it uses an optimized version \n",
    "    of the input for a quicker inference.\n",
    "\n",
    "    Taken from experiments/main.py from https://github.com/Felix-Petersen/difflogic\n",
    "    \"\"\"\n",
    "    orig_mode = model.training\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = np.mean(\n",
    "            [\n",
    "                (model(PackBitsTensor(x.to('cuda').reshape(x.shape[0], -1).round().bool())).argmax(-1) == y.to(\n",
    "                    'cuda')).to(torch.float32).mean().item()\n",
    "                for x, y in loader\n",
    "            ]\n",
    "        )\n",
    "        model.train(mode=orig_mode)\n",
    "    return res.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### LGN TRAINING  ###\n",
    "#####################\n",
    "# Define the model with the parameters in args\n",
    "llkw = dict(grad_factor=1., connections='unique')\n",
    "\n",
    "# Dimensions of the entry vector\n",
    "in_dim = args['num_niveis']*args['img_size']*args['img_size']\n",
    "# n. of classes\n",
    "class_count = 2\n",
    "# Initialize list of layers\n",
    "logic_layers = []\n",
    "\n",
    "# Architecture of the model\n",
    "arch = 'randomly_connected'\n",
    "k = args['num_neurons']\n",
    "l = args['num_layers']\n",
    "\n",
    "# Define the Flatten layer (1st layer) of the model\n",
    "logic_layers.append(torch.nn.Flatten())\n",
    "# Define the hidden layers, all the same with the parameters defined in args\n",
    "logic_layers.append(LogicLayer(in_dim=in_dim, out_dim=k, **llkw))\n",
    "for _ in range(l - 1):\n",
    "    logic_layers.append(LogicLayer(in_dim=k, out_dim=k, **llkw))\n",
    "# Add the GroupSum at the end\n",
    "model = torch.nn.Sequential(\n",
    "    *logic_layers,\n",
    "    GroupSum(class_count, args['tau'])\n",
    ")\n",
    "\n",
    "total_num_neurons = sum(map(lambda x: x.num_neurons, logic_layers[1:-1]))\n",
    "print(f'total_num_neurons={total_num_neurons}')\n",
    "total_num_weights = sum(map(lambda x: x.num_weights, logic_layers[1:-1]))\n",
    "print(f'total_num_weights={total_num_weights}')\n",
    "if args['id'] is not None:\n",
    "    results.store_results({\n",
    "        'total_num_neurons': total_num_neurons,\n",
    "        'total_num_weights': total_num_weights,\n",
    "    })\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.implementation = 'cuda'\n",
    "\n",
    "print(model)\n",
    "\n",
    "if args['id'] is not None:\n",
    "    results.store_results({'model_str': str(model)})\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Define optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
    "# Print the n. of trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Total trainable params:' + str(pytorch_total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary variables for training\n",
    "# Validation of previous epoch\n",
    "val_anterior = -1.0\n",
    "# Validation counter\n",
    "val_counter = 0\n",
    "# Initializes the list of validations to be saved during training\n",
    "val_list = []\n",
    "train_list = []\n",
    "test_list = []\n",
    "loss_list = []\n",
    "seconds_list = []\n",
    "cumu_training_seconds = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# We loop through the whole range of epochs\n",
    "for epoch in range(args['epochs']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('Epoch: ' + str(epoch))\n",
    "    # In each epoch we iterate through the whole training set 1 batch at the time\n",
    "    for i, (x, y) in tqdm(\n",
    "            enumerate(train_loader),\n",
    "            desc='Epoch',\n",
    "            total=len(train_loader),\n",
    "    ):\n",
    "        \n",
    "        x = x.to(torch.float64).to('cuda')\n",
    "        y = y.to('cuda')   \n",
    "        # For each batch we compute the loss \n",
    "        loss = train(model, x, y, loss_fn, optim)\n",
    "\n",
    "    cumu_training_seconds = cumu_training_seconds + (time.time() - start_time)\n",
    "    seconds_list.append(round(cumu_training_seconds,0))\n",
    "\n",
    "    print('Computing validation accuracy...')\n",
    "    val_accuracy_eval_mode = eval(model, val_loader, mode=False)\n",
    "    val_list.append(val_accuracy_eval_mode)\n",
    "    train_accuracy_eval_mode = eval(model, train_loader, mode=False)\n",
    "    train_list.append(train_accuracy_eval_mode)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    print('Loss: ' + str(np.round(loss,4)))\n",
    "    print('Val Acc: ' + str(np.round(val_accuracy_eval_mode,4)))\n",
    "    print('Train Acc: ' + str(np.round(train_accuracy_eval_mode,4)))\n",
    "    \n",
    "    # Compare the actual validation with the previous validation\n",
    "    if (val_accuracy_eval_mode <= val_anterior) or (epoch == args['epochs']-1):\n",
    "        # When the val_accuracy of these epoch is smaller than the previous one, the val_counter will be incremented by 1\n",
    "        val_counter = val_counter + 1\n",
    "        if (val_counter > args['patience']) or (epoch == args['epochs']-1):\n",
    "            # Stops the train and gets the best model until now\n",
    "            pickle.dump(best_model, open(os.path.join(path, 'epoch-{}'.format(best_epoch)), 'wb'))\n",
    "            # Saves the log file\n",
    "            with open(os.path.join(path, 'log_seconds.txt'), 'w') as file:\n",
    "                file.write(json.dumps(seconds_list))  \n",
    "            with open(os.path.join(path, 'log_val.txt'), 'w') as file:\n",
    "                file.write(json.dumps(val_list)) \n",
    "            with open(os.path.join(path, 'log_train.txt'), 'w') as file:\n",
    "                file.write(json.dumps(train_list))  \n",
    "            # Terminate model training\n",
    "            break\n",
    "    else:\n",
    "        # If the val_accuracy for the current epoch is larger than the one for the previous epoch, the pacience will be reseted\n",
    "        val_counter = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    val_anterior = val_accuracy_eval_mode if val_accuracy_eval_mode >= val_anterior else val_anterior\n",
    "# Load the best model from file and evaluate on the test set\n",
    "model = pickle.load(open(os.path.join(path, 'epoch-{}'.format(best_epoch)), 'rb'))\n",
    "test_accuracy_eval_mode = eval(model, test_loader, mode=False)\n",
    "print('Test Acc: ' + str(np.round(test_accuracy_eval_mode,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the best model from the folder and evaluate\n",
    "#best_lgn_model = pickle.load(open(os.path.join('trained_models/20241117_125109/epoch-46'), 'rb'))\n",
    "#eval(best_lgn_model, test_loader, mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure for Chapter 8: evolution of train_acc and val_acc during training\n",
    "# Sample data\n",
    "x_values = [*range(0, len(val_list), 1)] # x-axis values\n",
    "# Create traces for each curve\n",
    "trace1 = go.Scatter(x = x_values, y = val_list , mode = 'lines', name = \"Validation Accuracy\")\n",
    "trace2 = go.Scatter(x = seconds_list, y = train_list , mode = 'lines', name = \"Train Accuracy\", xaxis = \"x2\")\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "# Configure primary x-axis\n",
    "fig.update_xaxes(\n",
    "    title_text = \"Epochs\",\n",
    "    side = \"bottom\"\n",
    ")\n",
    "# Configure secondary x-axis with a different scale, overlaid on the primary axis\n",
    "fig.update_layout(\n",
    "    xaxis2 = dict(\n",
    "        title = \"Elapsed time [s]\",\n",
    "        overlaying =\"x\",             # Overlays on the same position as the primary x-axis\n",
    "        side =\"top\",                 # Displays at the top\n",
    "        ticktext = seconds_list,     # Shows alternative values        \n",
    "    ),\n",
    "    yaxis_title = \"Accuracy\"\n",
    ")\n",
    "# Show plot\n",
    "fig.write_image(os.path.join(path, 'val_train_acc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "### FFN TRAINING  ###\n",
    "#####################\n",
    "# Define the network's parameters\n",
    "args_ff = {\n",
    "    'epochs': 200,\n",
    "    'batch_size': 32,\n",
    "    'hidden_dim': 1150,\n",
    "    'num_layers': 4,\n",
    "    'learning_rate' : 0.00001,\n",
    "    'patience':5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the files into Dataloaders\n",
    "train_loader_ff = torch.utils.data.DataLoader(train_images_ff, batch_size=args_ff['batch_size'], shuffle=True, drop_last=True, num_workers = 4)\n",
    "test_loader_ff  = torch.utils.data.DataLoader(test_images_ff, batch_size=int(1e6), shuffle=False, num_workers = 4)\n",
    "val_loader_ff   = torch.utils.data.DataLoader(val_images_ff, batch_size=int(1e6), shuffle=False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Dimensions of the pictures in the dataset\n",
    "dim_in = args['img_size']*args['img_size']\n",
    "# n. of classes\n",
    "num_classes = 2\n",
    "# Define the layers of the FFN\n",
    "ff_layers = []\n",
    "\n",
    "ff_layers.append(nn.Flatten())\n",
    "\n",
    "first_l = nn.Linear(dim_in, args_ff['hidden_dim'])\n",
    "torch.nn.init.normal_(first_l.weight, mean=0.0, std=1.0)\n",
    "torch.nn.init.zeros_(first_l.bias)\n",
    "\n",
    "ff_layers.append(first_l)\n",
    "ff_layers.append(nn.ReLU())\n",
    "\n",
    "for _ in range(args_ff['num_layers'] - 1):\n",
    "    lay = nn.Linear(args_ff['hidden_dim'], args_ff['hidden_dim'])\n",
    "    torch.nn.init.normal_(lay.weight, mean=0.0, std=1.0)\n",
    "    torch.nn.init.zeros_(lay.bias)\n",
    "    ff_layers.append(lay)\n",
    "    ff_layers.append(nn.ReLU())\n",
    "\n",
    "last_l = nn.Linear(args_ff['hidden_dim'],num_classes)\n",
    "torch.nn.init.normal_(last_l.weight, mean=0.0, std=1.0)\n",
    "torch.nn.init.zeros_(last_l.bias)\n",
    "ff_layers.append(last_l)\n",
    "\n",
    "model = nn.Sequential(*ff_layers).to('cuda')\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Define the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=args_ff['learning_rate'])\n",
    "# Print model\n",
    "print(model)\n",
    "# Print the n. of trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Total trainable params:' + str(pytorch_total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the accuracy of the network\n",
    "def eval_fn(loader, model, mode):\n",
    "    orig_mode = model.training\n",
    "    with torch.no_grad():\n",
    "        model.train(mode=mode)\n",
    "        res = np.mean(\n",
    "            [\n",
    "                (model(x.to('cuda').float()).argmax(-1) == y.to('cuda')).to(torch.float32).mean().item()\n",
    "                for x, y in loader\n",
    "            ]\n",
    "        )\n",
    "        model.train(mode=orig_mode)\n",
    "    return res.item()\n",
    "\n",
    "# Define necessary variables for training\n",
    "# Validation of previous epoch\n",
    "val_anterior = -1.0\n",
    "# Initializes the list of validations to be saved during training\n",
    "val_list = []\n",
    "train_list = []\n",
    "seconds_list = []\n",
    "cumu_training_seconds = 0\n",
    "best_epoch = 0\n",
    "# Val counter\n",
    "val_counter = 0\n",
    "\n",
    "for epoch in range(args_ff['epochs']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('Epoch: ' + str(epoch))\n",
    "    for i, (x, y) in tqdm(\n",
    "            enumerate(train_loader_ff),\n",
    "            desc='Epoch',\n",
    "            total=len(train_loader_ff),\n",
    "    ):\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')    \n",
    "\n",
    "        outputs = model(x.float())\n",
    "        loss = loss_fn(outputs,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()     \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()  \n",
    "\n",
    "    cumu_training_seconds = cumu_training_seconds + (time.time() - start_time)\n",
    "    seconds_list.append(round(cumu_training_seconds,0))\n",
    "        \n",
    "    val_accuracy_eval_mode = eval_fn(val_loader_ff, model, mode=False)\n",
    "    val_list.append(val_accuracy_eval_mode)\n",
    "    train_acc = eval_fn(train_loader_ff, model, mode=False)\n",
    "    train_list.append(train_acc)\n",
    "\n",
    "    # Compare the actual validation with the previous validation\n",
    "    if (val_accuracy_eval_mode <= val_anterior) or (epoch == args_ff['epochs']-1):\n",
    "        # When the val_accuracy of these epoch is smaller than the previous one, the val_counter will be incremented by 1\n",
    "        val_counter = val_counter + 1\n",
    "        if (val_counter > args_ff['patience']) or (epoch == args_ff['epochs']-1):\n",
    "            # Stops the train and gets the best model until now\n",
    "            pickle.dump(best_model, open(os.path.join(path_ffn, 'epoch-{}'.format(best_epoch)), 'wb'))\n",
    "            # Saves the log file\n",
    "            with open(os.path.join(path_ffn, 'log_val.txt'), 'w') as file:\n",
    "                file.write(json.dumps(val_list)) \n",
    "            with open(os.path.join(path_ffn, 'log_args.txt'), 'w') as file:\n",
    "                file.write(json.dumps(args_ff)) \n",
    "            with open(os.path.join(path_ffn, 'log_train.txt'), 'w') as file:\n",
    "                file.write(json.dumps(train_list))  \n",
    "            with open(os.path.join(path_ffn, 'log_seconds.txt'), 'w') as file:\n",
    "                file.write(json.dumps(seconds_list))  \n",
    "            break\n",
    "    else:\n",
    "        # If the val_accuracy for the current epoch is larger than the one for the previous epoch, the pacience will be reseted\n",
    "        val_counter = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_epoch = epoch\n",
    "\n",
    "    val_anterior = val_accuracy_eval_mode if val_accuracy_eval_mode >= val_anterior else val_anterior\n",
    "    \n",
    "    print(f'Loss: {loss.item():.4f}'.format(loss))\n",
    "    print(f'Val Acc: {val_accuracy_eval_mode:.4f}'.format(val_accuracy_eval_mode))\n",
    "    print(f'Train Acc: {train_acc:.4f}'.format(train_acc))\n",
    "\n",
    "model = pickle.load(open(os.path.join(path_ffn, 'epoch-{}'.format(best_epoch)), 'rb'))\n",
    "test_accuracy_eval_mode = eval_fn(test_loader_ff, model, mode=False)\n",
    "print('Test Acc: ' + str(np.round(test_accuracy_eval_mode,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the best model from the folder and evaluate\n",
    "#best_ffn_model = pickle.load(open(os.path.join('trained_models_ffn/20241117_125109/epoch-13'), 'rb'))\n",
    "#eval_fn(test_loader_ff, best_ffn_model, mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure for Chapter 8: evolution of train_acc and val_acc during training\n",
    "# Sample data\n",
    "x_values = [*range(0, len(val_list), 1)] # x-axis values\n",
    "# Create traces for each curve\n",
    "trace1 = go.Scatter(x = x_values, y = val_list , mode = 'lines', name = \"Validation Accuracy\")\n",
    "trace2 = go.Scatter(x = seconds_list, y = train_list , mode = 'lines', name = \"Train Accuracy\", xaxis = \"x2\")\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "# Configure primary x-axis\n",
    "fig.update_xaxes(\n",
    "    title_text = \"Epochs\",\n",
    "    side = \"bottom\"\n",
    ")\n",
    "# Configure secondary x-axis with a different scale, overlaid on the primary axis\n",
    "fig.update_layout(\n",
    "    xaxis2 = dict(\n",
    "        title = \"Elapsed time [s]\",\n",
    "        overlaying =\"x\",           # Overlays on the same position as the primary x-axis\n",
    "        side =\"top\",               # Displays at the top\n",
    "        ticktext = seconds_list,   # Shows alternative values       \n",
    "    ),\n",
    "    yaxis_title = \"Accuracy\"\n",
    ")\n",
    "# Show plot\n",
    "fig.write_image(os.path.join(path_ffn, 'val_train_acc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Script for train and inference of an LGN and an FFN  \n",
    "\n",
    "Inference and training functions based on /experiments/main.py from https://github.com/Felix-Petersen/difflogic\n",
    " \n",
    "Author: Filipa Trindade\n",
    "\n",
    "BOOLEAN ALGEBRA: FROM DIGITAL CIRCUITS TO DEEP LEARNING APPLICATIONS Â© 2024 by Filipa Trindade Coito is licensed under CC BY-SA 4.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
